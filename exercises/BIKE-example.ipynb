{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using data from: \n",
    "https://archive.ics.uci.edu/dataset/560/seoul+bike+sharing+demand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cols = [\"bike_count\", \"hour\", \"temp\", \"humidity\", \"wind\", \"visibility\", \"dew_pt_temp\", \"radiation\", \"rain\", \"snow\", \"functional\"]\n",
    "df = pd.read_csv(\"../data_files/SeoulBikeData.csv\").drop([\"Date\", \"Holiday\", \"Seasons\"], axis=1) # removing some columns that we don't care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = dataset_cols\n",
    "df[\"functional\"] = (df[\"functional\"] == \"Yes\").astype(int) # converting yes to 1\n",
    "df = df[df[\"hour\"] == 12] # looking only at noon\n",
    "df = df.drop([\"hour\"], axis = 1) # dropping the hour column, since all = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we can look at all the data and see which have a linear looking plot\n",
    "# we are looking for data that does not seem too helpful, and then we can drop it\n",
    "for label in df.columns[1:]: # everything from temperature onward\n",
    "    plt.scatter(df[label], df[\"bike_count\"])\n",
    "    plt.title(label)\n",
    "    plt.ylabel(\"Bike count at noon\")\n",
    "    plt.xlabel(label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"wind\", \"visibility\", \"functional\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / validate test\n",
    "Now, we'll split this data into different sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = np.split(df.sample(frac = 1), [int(0.6 * len(df)), int(0.8 * len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xy(dataframe, y_label, x_labels = None): # used to extract data for just whatever youre interested in\n",
    "    dataframe = copy.deepcopy(dataframe) # copies entire dataframe\n",
    "    if x_labels is None: \n",
    "        X = dataframe[[c for c in dataframe.columns if c != y_label]].values\n",
    "    else: \n",
    "        if len(x_labels) == 1: \n",
    "            X = dataframe[x_labels[0]].values.reshape(-1,1)\n",
    "        else: \n",
    "            X = dataframe[x_labels].values\n",
    "\n",
    "    y = dataframe[y_label].values.reshape(-1,1)\n",
    "    data = np.hstack((X, y))\n",
    "\n",
    "    return data, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X_train_temp, y_train_temp = get_xy(train, \"bike_count\", x_labels = [\"temp\"])\n",
    "_, X_val_temp, y_val_temp = get_xy(val, \"bike_count\", x_labels = [\"temp\"])\n",
    "_, X_test_temp, y_test_temp = get_xy(test, \"bike_count\", x_labels = [\"temp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_reg = LinearRegression()\n",
    "temp_reg.fit(X_train_temp, y_train_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_reg.score(X_test_temp, y_test_temp) # pretty garbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train_temp, y_train_temp, label = \"Data\", color = \"blue\")\n",
    "x = tf.linspace(-20, 40, 100)\n",
    "plt.plot(x, temp_reg.predict(np.array(x).reshape(-1,1)), label = \"Fit\", color = \"red\", linewidth = 3)\n",
    "plt.legend()\n",
    "plt.title(\"Bikes vs. temp\")\n",
    "plt.ylabel(\"Num of bikes\")\n",
    "plt.xlabel(\"Temp\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X_train_all, y_train_all = get_xy(train, \"bike_count\", x_labels = df.columns[1:])\n",
    "_, X_val_all, y_val_all = get_xy(val, \"bike_count\", x_labels = df.columns[1:])\n",
    "_, X_test_all, y_test_all = get_xy(test, \"bike_count\", x_labels = df.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reg = LinearRegression()\n",
    "all_reg.fit(X_train_all, y_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reg.score(X_test_all, y_test_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with NN\n",
    "Here we use tensor flow again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.xlabel('Epoch') # Epoch is a training cycle, so we plot loss over training cycles\n",
    "    plt.ylabel('MSE')\n",
    "    plt.legend\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_normalizer = tf.keras.layers.Normalization(input_shape = (1,), axis = None)\n",
    "temp_normalizer.adapt(X_train_temp.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_nn_model = tf.keras.Sequential([\n",
    "    temp_normalizer, \n",
    "    tf.keras.layers.Dense(1) # using a single node is linear\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_nn_model.compile(optimizer = tf.keras.optimizers.legacy.Adam(learning_rate = 0.1), loss = \"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = temp_nn_model.fit(\n",
    "    X_train_temp.reshape(-1), y_train_temp, \n",
    "    verbose = 0, \n",
    "    epochs = 1000, \n",
    "    validation_data = (X_val_temp, y_val_temp)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fit here will be slightly different, as we are using back propagation to train a neural net node\n",
    "# That differs from the previous, which simply tries to compute the line of best fit\n",
    "plt.scatter(X_train_temp, y_train_temp, label = \"Data\", color = \"blue\")\n",
    "x = tf.linspace(-20, 40, 100)\n",
    "plt.plot(x, temp_nn_model.predict(np.array(x).reshape(-1,1)), label = \"Fit\", color = \"red\", linewidth = 3)\n",
    "plt.legend()\n",
    "plt.title(\"Bikes vs. temp\")\n",
    "plt.ylabel(\"Num of bikes\")\n",
    "plt.xlabel(\"Temp\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net\n",
    "Here we'll add more nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_normalizer = tf.keras.layers.Normalization(input_shape = (1,), axis = None)\n",
    "temp_normalizer.adapt(X_train_temp.reshape(-1))\n",
    "\n",
    "nn_model = tf.keras.Sequential([\n",
    "    temp_normalizer, \n",
    "    tf.keras.layers.Dense(32, activation = 'relu'), \n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'relu')\n",
    "])\n",
    "nn_model.compile(optimizer = tf.keras.optimizers.legacy.Adam(learning_rate = 0.001), loss = \"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = nn_model.fit(\n",
    "    X_train_temp, y_train_temp,\n",
    "    validation_data = (X_val_temp, y_val_temp), \n",
    "    verbose = 0, epochs = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the fit will be non-linear!\n",
    "plt.scatter(X_train_temp, y_train_temp, label = \"Data\", color = \"blue\")\n",
    "x = tf.linspace(-20, 40, 100)\n",
    "plt.plot(x, nn_model.predict(np.array(x).reshape(-1,1)), label = \"Fit\", color = \"red\", linewidth = 3)\n",
    "plt.legend()\n",
    "plt.title(\"Bikes vs. temp\")\n",
    "plt.ylabel(\"Num of bikes\")\n",
    "plt.xlabel(\"Temp\")\n",
    "plt.show()\n",
    "\n",
    "# Clearly you can tell that the model still is not perfect, particularly in the leftward region where there is no data. \n",
    "# It would have been better to remove this section from the NN altogether, probably. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_normalizer = tf.keras.layers.Normalization(input_shape = (6,), axis = -1)\n",
    "all_normalizer.adapt(X_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = tf.keras.Sequential([\n",
    "    all_normalizer, \n",
    "    tf.keras.layers.Dense(32, activation = 'relu'), \n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'relu')\n",
    "])\n",
    "nn_model.compile(optimizer = tf.keras.optimizers.legacy.Adam(learning_rate = 0.001), loss = \"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = nn_model.fit(\n",
    "    X_train_all, y_train_all,\n",
    "    validation_data = (X_val_all, y_val_all), \n",
    "    verbose = 0, epochs = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the mean squared error for the linear regressor vs. neural net\n",
    "y_pred_lr = all_reg.predict(X_test_all)\n",
    "y_pred_nn = nn_model.predict(X_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_pred, y_real):\n",
    "    return (np.square(y_pred - y_real)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE(y_pred_lr, y_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE(y_pred_nn, y_test_all)\n",
    "# nn gives a larger error!? LOL! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the predictions\n",
    "\n",
    "ax = plt.axes(aspect = \"equal\")\n",
    "plt.scatter(y_test_all, y_pred_lr, label = \"Lin Reg Preds\")\n",
    "plt.scatter(y_test_all, y_pred_nn, label = \"NN Preds\")\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "lims = [0, 1800]\n",
    "plt.xlim = lims\n",
    "plt.ylims = lims\n",
    "plt.legend()\n",
    "_ = plt.plot(lims, lims, c=\"red\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
